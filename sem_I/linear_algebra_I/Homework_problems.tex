\documentclass[english,10pt,a4paper,]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\title{\vspace{-2cm}\textbf{Linear Algebra I - Homework problems} \\
\author{Arkaraj Mukherjee}}
\begin{document}
\maketitle
{\it{If problem $\bf{n}$ from $\bf{exercise\_x.pdf}$ is assigned as homework then it will be referred to as $\bf{x.n}$.}}
$\\\noindent\rule{\linewidth}{0.5pt}\linebreak$
$\bf{1.8}\hspace{0.5pt}$ It was already shown in class that $\mathbb C^{\mathbb R}=\{f:\mathbb R\to\mathbb C\}$ forms a vectorspace over $\mathbb R$ with the same operations defined here. We show that $V$ is a subspace and hence a vectorspace itself. For all $\alpha\in\mathbb R$ and $f,g\in V$ then for all $t\in\mathbb R$, $$\overline{(\alpha f+g)(t)}=\overline{\alpha f(t)+g(t)}=\overline{\alpha}\cdot\overline{f(t)}+\overline{g(t)}=\alpha\cdot f(-t)+g(-t)=(\alpha f+g)(-t)$$and thus $\alpha f+g\in V$ and we are done as according to what was discussed in class, $V$ is a subspace if for all $\alpha\in\mathbb R$ and $f,g\in V$ its the case that $\alpha f+g\in V$ too. An example of a function with some non real outputs in $V$ is $f(t)=it.$
$\\\noindent\rule{\linewidth}{0.5pt}\linebreak$
$\bf{2.11}\hspace{0.5pt}$ We have seen that if $A\subseteq B$ are subsets of $V$ then $Sp(A)\subseteq Sp(B)$ and using this if $A,B$ are any subsets of $V$ (not the ones used to state the result earlier) that as $A,B\subseteq A\cup B$ we have that $Sp(A),Sp(B)\subseteq Sp(A\cup B)$ and thus $Sp(A)\cup Sp(B)\subseteq Sp(A\cup B)$ and the second bulleted claim follows similarly from the fact that $A\cap B\subseteq A,B$. The last claim is false, we can take disjoint sets $A,B$ such that $Sp(A)=Sp(B)\neq\{0\}$, for example in $V=\mathbb R^2$ take $A=\{(0,1),(1,0)\}$ and $B=\{(0,-1),(-1,0)\}$.
$\\\bf{2.17}\hspace{0.5pt}$ By definition, if $W_1+W_2=V$ then for all $v\in V$ there are some $w_1\in W_1$ and $w_2\in W_2$ such that $w_1+w_2=W$. We claim that these are unique. Say there are $(w_1,w_2),(u_1,u_2)\in W_1\times W_2$ such that $w_1+w_2=v=u_1+u_2$. Then we see that $w_1+w_2=u_1+u_2$ and thus $W_1\ni w_1-u_1=u_2-w_2\in W_2$ as these are vectorspaces, thus $w_1-u_1,u_2-w_2\in W_1\cap W_2$ as both of these are in both of these subsets. Now as $\{0\}=W_1\cap W_2$ we see that $w_1-u_1=0=w_2-u_2$ thus $w_1=u_1$ and $w_1=u_2$ thus the $w_1,w_2$ we get are unique.
$\\\noindent\rule{\linewidth}{0.5pt}\linebreak$
$\bf{3.9}$ As per the condition on $U$ we see that all $(x_1,x_2,x_3,x_4,x_5)\in U$ can be written in the form $(3x_2,x_2,7x_4,x_4,x_5)$ where $x_2,x_4,x_5\in\mathbb R$. It can be easily seen that $U$ is a subspace via the definiton and it is also spanned by the linearly independant set $\{(3,1,0,0,0),(0,0,7,1,0),(0,0,0,0,1)\}$ and hence this is a basis for $U$.
$\\\bf{3.11}$ If $\alpha\in\mathbb R$ then $\{x_1,x_2+\alpha x_1,\ldots,x_n+\alpha x_1\}$ is linearly independant and hence a basis iff for $\beta_1,\ldots,\beta_n\in\mathbb R$, $$\beta_1 x_1+\beta_2(x_2+\alpha x_1)+\ldots+\beta_n(x_n+\alpha x_1)=0\implies \forall k,\beta_k=0$$we can rewrite this as, $$(\beta_1+\alpha(\beta_2+\ldots+\beta_n))x_1+\beta_2x_2+\ldots+\beta_nx_n=0$$But as $\{x_1,\ldots,x_n\}$ was already a basis and hence linearly independant, we see that, $\beta_1+\alpha(\beta_2+\ldots+\beta_n)=\beta_2=\ldots=\beta_n=0$ and thus $\beta_1=-\alpha(0+\ldots+0)=0$ as well and thus this new set is linearly independant and thus also a basis being of size $n$. Now we can clearly take $\alpha$ large enough and get a basis where all the vectors have all positive coordinates.
$\\\noindent\rule{\linewidth}{0.5pt}\linebreak$
\textbf{4.5} For the only if part we see that if $S\oplus T_1=V=S\oplus T_2$ then $\dim(T_1)=\dim(T_2)=\dim(V)-\dim(S)$ by the modular law. For the sake of contradiction (we will contradict the fact that $T_1\cap T_2=\{0\}$) assume that $\dim S<\dim V/2$, this implies that $\dim T_1=\dim T_2>\dim V/2$. Now take some basis $\{a_1,\ldots,a_m\}$ for $T_1$ and another basis $\{b_1,\ldots,b_m\}$ for $T_2$, we now see that as the set $\{a_1,\ldots,a_m,b_1,\ldots,b_m\}$ has size strictly greater than $2\times\dim V/2=\dim V$ and thus it must be linearly dependant because the size of some linearly independant set is atmost that of some spanning set and in this case the basis with $\dim V$ elements spans $V$ so they have size atmost $\dim V$. We can thus find scalars $\alpha_1,\ldots,\alpha_m,\beta_1,\ldots,\beta_m$ not all zero such that 
$$\sum_{i=1}^m\alpha_ia_i+\sum_{i=1}^m\beta_ib_i=0\implies T_1\ni\sum_{i=1}^m\alpha_ia_i=\sum_{i=1}^m(-\beta_i)b_i\in T_2.$$ 
Here not all the scalars are zero so wlog say $\alpha_1\neq 0$, now this implies that the left side isn't zero itself as otherwise we would contradict the linear independance of elements of the basis of $T_1$ i.e. $\{a_1,\ldots,a_m\}$. Thus we found a nonzero vector $v=\sum_{i=1}^m\alpha_ia_i\in T_1\cap T_2$ which contradicts the fact that $T_1\cap T_2=\{0\}.$ From this we have that $\dim T_1=\dim T_2\leq\dim V/2$ which implies that $\dim S=\dim V-\dim T_1\geq \dim V/2.$ 

For the if part we will provide a construction of such $T_1$ and $T_2.$ First take some basis $\{v_1,\ldots,v_n\}$ for $S$ and extend it to a basis $\{v_1,\ldots,v_n,\ldots,v_{n+m}\}$ where $m(\leq n)$ might be zero, in which case taking $T_1=T_2=\{0\}$ suffices as $S$ is the whole space anyways and all the conditions hold. Now assume $m\geq 1$ and let $T_1$ be the subspace spanned by $\{v_{n+1},\ldots,v_{n+m}\}$. We can see that $T_1+S$ is clearly $V$. Now we prove that $T_1\cap S=\{0\}$. If we had some vector $v=\sum_{i=1}^n\alpha_iv_i=\sum_{i=n+1}^{n+m}\beta_iv_i\in S\cap T_1$ then 
$$\sum_{i=1}^n\alpha_iv_i+\sum_{j=n+1}^{n+m}(-\beta_j)v_j=0\implies\alpha_1=\ldots=\alpha_n=-\beta_{n+1}=\ldots=-\beta_{n+m}=0$$ 
by the linear independance of $\{v_1,\ldots,v_{n+m}\}$ as its a basis. This implies that $v=\sum_{i=1}^n\alpha_iv_i=\sum_{i=1}^n0\cdot v_i=\sum_{i=1}^n0=0$ and thus $S\cap T_1=\{0\}$ so by the definition of a direct sum, $S\oplus T_1=V.$ 

Now we claim that, the subspace $T_2$ spanned by $\{v_{n+1}+v_1,\ldots,v_{n+m}+v_m\}$ is also a complement of $S$ and that $T_1\cap T_2=\{0\}$. First we check that $S+T_2=V$. Take any vector $v=\sum_{k=1}^{n+m}\alpha_kv_k\in V$. Then we can rewrite this vector as  
$$\sum_{k=1}^m(\alpha_k-\alpha_{n+k})v_k+\sum_{m<k\leq n}\alpha_kv_k+\sum_{k=n+1}^{n+m}\alpha_k(v_k+v_{k-n}).$$ 
The first two sums are in $S$ and the last sum is in $T_2$, so every vector of $V$ can be expressed as an element of $S+T_2$, which shows that $S+T_2=V$.  

Now we check $S\cap T_2=\{0\}$. Suppose $x\in S\cap T_2$. Then we can write $x$ in two ways, once as $x=\sum_{i=1}^n\alpha_iv_i$ since $x\in S$ and again as $x=\sum_{k=1}^m\gamma_k(v_{n+k}+v_k)$ since $x\in T_2$. Comparing the coefficients in the basis $\{v_1,\ldots,v_{n+m}\}$ we see that for $v_{n+k}$ we must have $\gamma_k=0$ for all $k=1,\ldots,m$. Putting these back into the second expression we get $x=0$. Hence $S\cap T_2=\{0\}$.  

Finally we check $T_1\cap T_2=\{0\}$. Suppose $y\in T_1\cap T_2$. Then $y=\sum_{k=1}^m\beta_kv_{n+k}$ since $y\in T_1$ and also $y=\sum_{k=1}^m\gamma_k(v_{n+k}+v_k)$ since $y\in T_2$. Comparing coefficients of $v_k$ gives $\gamma_k=0$ for all $k=1,\ldots,m$, and thus the right hand side becomes $y=\sum_{k=1}^m0\cdot(v_{n+k}+v_k)=0$. Therefore $y=0$ and $T_1\cap T_2=\{0\}$.  

Thus we have shown that $S\oplus T_2=V$ and $T_1\cap T_2=\{0\}$ as required.
$\\\noindent\rule{\linewidth}{0.5pt}\linebreak$
$\bf{5.9}$ Let $A_1,\ldots,A_n$ be the columns of $A$, $C_1,\ldots,C_m$ be the columns of $C$ and $B_1,\ldots,B_m$ be the columns of $B$  then in block form we can write, 
$$\begin{bmatrix}
	A_1 & A_2 & \cdots & A_n & B_1 & B_2 & \cdots & B_m
	\\
	0 & 0 & \cdots & 0 & C_1 & C_2 & \cdots & C_m
\end{bmatrix}$$Assuming both the matrices $A$ and $C$ to have nonzero rank as if one of them having zero rank would mean that all the columns of said matrix is zero as otherwise all scalar multiples of some fixed nonzero column would be an one dimensional subspace of the column space making it have nonzero dimension and we can clearly see that,
$$\rho\begin{bmatrix}
	A & B 
	\\
	0 & 0
\end{bmatrix}
=\rho\begin{bmatrix}
	A & B
\end{bmatrix}
\geq\rho(A)=rho(A)+\rho(0)
$$
as adding column vectors to some set can not decrease the dimension of its span and similarly considering the rows (and the row rank which is equal to the column rank as proven in class)
$$\rho\begin{bmatrix}
	0 &  B
	\\
	0 & C
\end{bmatrix}
\geq\rho(C)=\rho(0)+\rho(C)$$
and the case $A=C=0$ is covered by the last case with $C=0.$
WLOG $A_1,\ldots,A_{\rho(A)}$ is a basis for the column space of $A$ (we can trim the spanning set which consists of all the columns of the matrix to a basis) making them linearly independant and choose $C_1,\ldots,C_{\rho(C)}$ similarly for $C$. Now for some scalars 
$\alpha_1,\ldots,\alpha_{\rho(A)},\beta_1,\ldots,\beta_{\rho(C)}$, $$\sum_{i=1}^{\rho(A)}\alpha_i\cdot\begin{bmatrix}A_i \\ 0\end{bmatrix}+\sum_{j=1}^{\rho(C)}\beta_j\cdot\begin{bmatrix}B_j \\ C_j\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^{\rho(A)}\alpha_i\cdot A_i+\sum_{j=1}^{\rho(C)}\beta_j\cdot B_j \\ \sum_{k=1}^{\rho(C)}\beta_k\cdot C_k\end{bmatrix}=0$$
$$\implies\sum_{i=1}^{\rho(C)}\beta_i\cdot C_i=0$$
Which implies that $\beta_i$ are all zero by linear independance which in turn implies that $\alpha_i$ are also zero after considering the other equation
$$\sum_{i=1}^{\rho(A)}\alpha_i\cdot A_i+\sum_{j=1}^{\rho(C)}\beta_j\cdot B_j=0\implying\sum_{i=1}^{\rho(A)}\alpha_i\cdot A_i=0$$
which forces all $\alpha_i$ to be zero as well through linear independance and all together, all the scalars are forced to be zero and hence we can conclude that
$$\left\{\begin{bmatrix} A_i\\ 0\end{bmatrix}:1\leq i\leq\rho(A)\right\}\bigcup\left\{\begin{bmatrix}B_j\\ C_j\end{bmatrix}:1\leq j\leq\rho(C)\right\}$$
is a linearly independant set of columns of our matrix and thus the matrix has rank atleast more than the size of said set which is $\rho(A)+\rho(C)$. For a case where strict inequality occurs we can consider,
$$\begin{bmatrix}A & B\\ 0 & C\end{bmatrix}=\begin{bmatrix}1 & 0 & 0 & 1\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\end{bmatrix}$$
	$$A=\begin{bmatrix}1 & 0\\ 0 & 0\end{bmatrix},B=\begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix},C=\begin{bmatrix} 0 & 0\\ 1 & 0\end{bmatrix}$$
Now to show that the rank of a upper triangular matrix is atleast as much as the number of nonzero diagonal elements we can proceed by induction with the base case being a $1\times 1$ matrix which is effectively a scalar and has rank $1$ if its nonzero and $0$ otherwise.
 Assume that this fact is true for all $k\times k$ upper triangular matrices when $k>n$, if we can prove this for $n$ then we can conclude by the principle of strong induction.
 To prove this, enumerate the elements of our $n\times n$ matrix as $(a_{i,j})$ and partition it into the following blockss
$$A=\{a_{i,j}\}_{\substack{1\leq i\leq 1\\ 1\leq j\leq 1}},B=\{a_{i,j}\}_{\substack{1\leq i\leq n\\ 1<j\leq n}},C=\{a_{i,j}\}_{\substack{1<i\leq n\\ 1<j\leq n}}$$
where the remaining block will be all zeroes by the virtue of the original matrix being upper triangular and we can write the matrix as,
$$\begin{bmatrix}A & B \\ 0 & C\end{bmatrix}$$
Using the induction hypothesis, as the size of upper triangular matrices $A,C$ are $1,n-1$ respectively, by the identity proved above the original matrix has rank atleast $\rho(A)+\rho(C)$ which is atleast $$\underbrace{\left|\{1\leq i\leq 1:a_{i,i}\neq 0\}\right|}_{\leq\rho(A)\text{ by induction hyp.}}+\underbrace{\left|\{1<j\leq n:a_{j,j}\neq 0\}\right|}_{\leq\rho(C)\text{ by induction hyp.}}=\left|\{1\leq i\leq n:a_{i,i}\neq 0\}\right|$$so we are done.
$\\ \bf{5.12}$ Firstly for every such $k$, by considering the column space of our matrix and trimming the spanning set which is the columns, to a basis which has size $r$, we can choose any $k$ columns from this set and these will form a matrix with $k$ linearly independant columns when concatenated. Again, on this $m\times k$ matrix, as the row rank which is equal to the column rank $k$ (there are $k$ columns, all of which are linearly independant and hence they have a span of dimension exactly $k$) we can repeat the exact arguement we gave to get $k$ columns, to now get $k$ rows in this matrix which are linearly independant giving us a $k\times k$ matrix which has row rank $r$ and thus column rank also the same and this is a submatrix of the original matrix as we only first omitted $n-r$ columns and then $m-r$ rows from it to arrive here.
\end{document}
