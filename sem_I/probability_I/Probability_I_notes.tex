\documentclass[english,10pt,a4paper,]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym,graphicx}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\title{\vspace{-2cm}\textbf{Probability I - Random Variables} \\
\author{Arkaraj Mukherjee}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\e}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\indep}{\rotatebox[origin=c]{180}{$\Pi$}}
\begin{document}
\maketitle
For a sample space $\Omega$, event space $\mathcal{F}$ and probability $P$, a \textit{random variable} is a function $X:\Omega\to\mathbb R$. the set of values the random variable can take, i.e. it's range is called the \textit{support} of the RV (short form of Random Variable) often denoted by $$\supp{X:}=\{X(\omega)\vert\omega\in\Omega\}$$RVs $X$ for which $\supp{X}$ is atmost countable i.e. countably infinite or finite are called \textit{discrete RVs}. The probabilities of the RV taking on some values in its support are governed by the \textit{probability mass function} (\textit{pmf.} in short) often denoted by $p$, i.e. $$p:\supp{X}\longrightarrow[0,1]$$ $$\hspace{72pt}x\mapsto P(X=x):=P(X^{-1}\{x\})$$In practice we extend this definition of $p$ on $\supp{X}$ all the way to $\mathbb{R}$ by fixing $p(x)=0$ for all $x\in\mathbb R\setminus\supp{X}$. For discrete RVs, if we ennumerate $\supp{X}=\{x_i\vert i=1,2,\ldots\}$ then from the axioms of probability we get that, $$\sum_{i=1}^{\infty}p(x_i)=\underbrace{\sum_{i=1}^{\infty}P(X^{-1}(\{x_i\}))=P\left(\bigcup_{i=1}^{\infty}X^{-1}(\{x_i\})\right)=P(\Omega)=1}_{\text{Why are these three equalities true?}}$$
The $\textit{cumulative distribution function}$ (cdf. for short) of a random variable is a function on the real line defined as, $$F:\mathbb R\to[0,1]$$ $$\hspace{134pt}x\mapsto P(X\leq x):=P(X^{-1}((-\infty,x]))$$This is a non-decreasing step-function with jump discontinuitites and from the definition we can see that (try proving it) $$p(x)=F(x)-\lim_{\substack{h\to x\\ h<x}}F(h)$$The limit on the right is sometimes known as the \textit{left limit of $F$ at $x$}. From the axioms of probability again we can see that $$F(x)=P(X^{-1}((-\infty,x]))=P\left(\bigcup_{x_i<x}X^{-1}(\{x_i\})\right)=\sum_{x_i<x}P(X^{-1}(\{x_i\}))=\sum_{x_i<x}p(x_i)$$You should notice that we can also deduce the equation with the left limit from this as well. The \textit{expectation} of a RV is a weighted average of the values it takes, formally this is written as$$\mathbb{E}[X]:=\sum_{i=1}^{\infty}x_ip(x_i)$$It may not always exist(i.e. converge). Suppose we have a RV $X$, then for some function $f$ on $\supp X$, $Y=f(X)$ is also a RV i.e. a function $\Omega\to\mathbb R$ and $\supp Y=f(\supp X)$. Also, $$\e{Y}=\sum_{y\in\supp Y}yP(f(X)=y)=\sum_{y\in\supp Y}\sum_{x\in f^{-1}(y)}yP(X=x)$$ $$=\sum_{y\in \supp Y}\sum_{x\in f^{-1}(y)}f(x)P(X=x)=\sum_{x\in\bigcup_{y\in f(\supp X)}f^{-1}(y)}f(x)P(X=x)$$ $$=\sum_{x\in\supp X}f(x)P(X=x)$$This is a major result. The \textit{k-th moment} of a RV is defined as $\e{X^k}:=\e{g(X)}$ where $g:x\mapsto x^k$. Let $X,Y$ be RVs on the same sample space, then we see that when we define $(X+Y)(\omega):=X(\omega)+Y(\omega)$ then, $$\e{X+Y}=\sum_{x\in\supp X}\sum_{y\in\supp Y}(x+y)P(X=x\land Y=y)$$ $$=\sum_{x\in\supp X}x\sum_{y\in \supp Y}P(X=x\land Y=y)+\sum_{y\in\supp Y}y\sum_{x\in \supp X}P(X=x\land Y=y)$$ $$=\sum_{x\in\supp X}xP(X=x)+\sum_{y\in \supp Y}yP(Y=y)=\e{X}+\e{Y}$$and clearly for constant $c$ we have $\e{cX}=c\e{X}$ thus expectation is linear. For subsets $A\subseteq\Omega$ we define an \textit{indicator RV} $I_A$ as the RV that is one on $A$ and $0$ everywhere else, this clearly implies that $\e{I_A}=P(A).$ The \textit{mean absolute deivation (MAD)} is defined as $\e{|X-\e{X}|}$, the \textit{variance} is defined as $\var{X}:=\e{\left(X-\e{X}\right)^2}$ and the \textit{standard deviation} is $\sqrt{\var{X}}.$ Clearly $\var{X}\geq 0$ and $\var{X}=\e{X}^2-\e{X^2}$ thus, $\e{X}^2\geq\e{X^2}$. Also $\var{aX+b}=a^2\var{X}.$ The \textit{degenerate probability distribution} is where for some $x_o\in\mathbb R$ we have $\supp X=\{x_o\}$, for this $p(x)=\delta_{x,x_o}$ and $F(x)=0$ if $x<x_o$ and $1$ otherwise, $\e{X}=x_o$ and $\var{X}=0.$ The \textit{discrete uniform} distribution is where $\supp X=\{a,a+1,\ldots,b\}$ for some $a\leq b$ where we have $p(x)=1/|\supp X|=1/(b-a+1)$ on its support and zero elsewhere. $F(x)$ is zero for $x<a,$ $|(-\infty,x]\cap\supp X|/|\supp X|=(\lfloor x\rfloor-a+1)/(b-a+1)$ on $[a,b]$ and $1$ elsewhere. Here, $\e{X}=(a+b)/2$ and $\var{X}=\var{x-(a-1)}=((b-a+1)^2-1)/12\asymp |b-a|^2.$ For some $p\in[0,1]$ the \textit{bernoulli distribution}, $X\sim\operatorname{Ber}(p)$ is where $\supp X=\{0,1\}$ with $p(1)=p$ and $p(0)=1-p=(\text{usually written as } q)$, $F(x)=0$ if $x<0$, $q$ on $[0,1)$ and $1$ elsewhere. The \textit{binomial distribution}, $X\sim\operatorname{Bin}(n,p)$ for $n\in\mathbb N$ and $p\in[0,1]$ is where $\supp X=\{0,1,\ldots,n\}$ and $p(x)=\binom{n}{x}p^x(1-p)^{n-x}$ on its support and zero elsewhere but here $F(x)$ doesn't have a closed form. We say that two RVs $X,Y$ are independent and write $X\indep Y$ iff for all $x,y$ $P(X=x\land y=y)=P(X=x)P(Y=y).$ Now for discrete RVs $X,Y$ such that $X\indep Y$ we can see that for any $A\subseteq\supp X$ and $B\subseteq\supp Y$ we have that, $$P(X\in A\land Y\in B)=P\left(\bigcup_{x\in A,y\in B}X^{-1}(\{x\})\cap Y^{-1}(\{y\})\right)=\sum_{x\in A}\sum_{y\in B}P(X=x\land Y=y)$$ $$=\sum_{x\in A}\sum_{y\in B}p_X(x)p_Y(y)=\left(\sum_{x\in A}p_X(x)\right)\cdot\left(\sum_{y\in B}p_Y(y)\right)=P(X\in A)\cdot P(Y\in B)$$using usual theorems from absolute convergence of $\Sigma_{x\in A}p_X(x)$ etc. This tells us that if $X\indep Y$ then any events defined with $X,Y$ are also independent. Now we see that if (ignore the fact that the sum of random variables isn't defined for this to work yet, this course sucks) $X_1,\ldots,X_n\sim \operatorname{Ber}(p)$ are independent then $Y=\Sigma X_i\sim\operatorname{Bin}(n,p)$. For a proof,we can see that $Y$ has support $\{0,\ldots,n\}$ with $p_Y(k)=P(\text{k of the }X_i\text{'s are 1 and the rest are zero})=\binom{n}{k}p^k(1-p)^{n-k}$. Now if $X\sim\operatorname{Bin}(n,p)$ and $Y\sim\operatorname{Bin}(m,p)$ are independent then we have that, $X+Y\sim\operatorname{Bin}(m+n,p)$. This can be done in two ways, take independent bernoulli RVs $Z_1,\ldots,Z_{m+n}\sim\operatorname{Ber}(p)$ and then, $X=\sum_{i=1}^nZ_i$ and $Y=\sum_{i=1+1}^mZ_i$ giving us that $X+Y=\sum_{i=1}^{n+m}Z_i\sim\operatorname{Bin(n,p)}$. Apart from this another way is to see that, $$p_{X+Y}(k)=\sum_{j\geq 0}P(X=j\land Y=k-j)=\sum_{j\geq 0}p_X(j)p_Y(k-j)$$ $$=\sum_{j\geq 0}\binom{n}{j}\binom{m}{k-j}p^{j+(k-j)}(1-p)^{n-j+(m-(k-j))}$$ $$=\sum_{j\geq 0}\binom{n}{j}\binom{m}{k-j}p^k(1-p)^{n+m-k}=[z^k]\left((1+z)^m(1+z)^n\right)p^k(1-p)^{n+m-k}$$ $$=\binom{n+m}{k}p^k(1-p)^{n+m-k}$$where we have used the convention that $\binom{a}{b}=0$ for $a<b$ and arrive at the same conclusion.
\end{document}